The naming format follows a structured convention to indicate dataset characteristics:
- **full_data_v2**: The name of the dataset.  
- **train/test/all**: Specifies that this is a train/test/non-splited set.  
- **full**: Includes all formal language ↔ natural language pairs.  
- **eng**: Includes only formal language ↔ English pairs.  
- **agda**: Includes only Agda ↔ natural language pairs.  
- **agda_eng**: Includes only Agda ↔ English pairs.  
- **small**: A subset of the dataset containing 10,000 training samples or 1,000 test samples.  

| Experiment Group | Training Data Coverage | Purpose |
|------------------|------------------------|---------|
| A | Each FL (Dedukti/Agda/Coq/Lean) ↔ English trained independently (4 models) | Baseline: single formal language × English |
| B | All FL ↔ English combined training (1 model) | Verify the effect of joint training of multiple formal languages |
| C | Agda ↔ Each NL (English/French/Swedish) trained independently (3 models) | Baseline: single natural language × Agda |
| D | Agda ↔ All NL combined training (1 model) | Verify the effect of joint training of multiple natural languages |
| E | All FL ↔ All NL (4×3 total 12 pairs) combined training | Full multi-pair model |
| F | Only Agda ↔ English, but split into smaller data-scarce scenarios (e.g., 10%, 1%) for few-shot training | Verify the effect of data quantity vs. accuracy |

**Base Model Qwen2.5-7B-Instruct**  
- **Base Line**   
**On full_data_base_test_agda_eng_single (351)**   
    "predict_bleu-4": 51.19749373219373,  
    "predict_model_preparation_time": 0.0047,  
    "predict_rouge-1": 75.08338774928774,  
    "predict_rouge-2": 53.35914900284901,  
    "predict_rouge-l": 60.117741880341875,  
    "predict_runtime": 1310.4267,  
    "predict_samples_per_second": 0.268,  
    "predict_steps_per_second": 0.134  
    "syntax_error": 294  

- **Full Data v2 with All Formal Languages and First 1 Expression of All Nature Languages (38,736)**   
**Training (Epochs: 1, Steps: 2,421)**  
**Evaluation**  
**On full_data_v2_base_agda_eng_first1 (359)**   
    "predict_bleu-4": 98.36456768802228,  
    "predict_model_preparation_time": 0.0045,  
    "predict_rouge-1": 99.3067044568245,  
    "predict_rouge-2": 98.9396459610028,  
    "predict_rouge-l": 98.80686935933149,  
    "predict_runtime": 848.1061,  
    "predict_samples_per_second": 0.423,  
    "predict_steps_per_second": 0.212  
    "syntax_error": 14   
- **Full Data v2 with All Formal Language and First 1 Expression of English (12,912)**   
**Training (Epochs: 3, Steps: 2,421)**  
**Evaluation**  
**On full_data_v2_base_agda_eng_first1 (359)**   
    "predict_bleu-4": 98.6277417827298,  
    "predict_model_preparation_time": 0.0043,  
    "predict_rouge-1": 99.47865933147631,  
    "predict_rouge-2": 99.12625543175488,  
    "predict_rouge-l": 98.98778830083566,  
    "predict_runtime": 827.557,  
    "predict_samples_per_second": 0.434,  
    "predict_steps_per_second": 0.218  
    "syntax_error": 13  
- **Full Data v2 with Only Agda and First 1 Expression of All Nature Languages (9,684)**   
**Training (Epochs: 4, Steps: 2,421)**  
**Evaluation**  
**On full_data_v2_base_agda_eng_first1 (359)**   
    "predict_bleu-4": 98.97689247910864,  
    "predict_model_preparation_time": 0.0043,  
    "predict_rouge-1": 99.64472005571031,  
    "predict_rouge-2": 99.48186629526462,  
    "predict_rouge-l": 99.32096462395543,  
    "predict_runtime": 868.0276,  
    "predict_samples_per_second": 0.414,  
    "predict_steps_per_second": 0.207  
    "syntax_error": 17  
- **Full Data v2 with Only Agda and First 1 Expression of English (3,228)**   
**Training (Epochs: 12, Steps: 2,421)**  
**Evaluation**  
**On full_data_v2_base_agda_eng_first1 (359)**   
    "predict_bleu-4": 98.91109192200555,  
    "predict_model_preparation_time": 0.0046,  
    "predict_rouge-1": 99.54159860724235,  
    "predict_rouge-2": 99.26295292479108,  
    "predict_rouge-l": 99.27269999999999,  
    "predict_runtime": 874.7486,  
    "predict_samples_per_second": 0.41,  
    "predict_steps_per_second": 0.206   
    "syntax_error": 19  

![alt text](/main/results/Qwen2.5-7B/training_logs/full_data_v2_step-loss%20curves.png)  

----------------------------------------------------------

- **Parallel-informath with All Formal Languages and All Nature Languages Expressions (14,748)**   
**Training (Epochs: 2.5, Steps: 2,303)**  
**Evaluation**  
**On parallel-informath_test_agda_eng (874)**   
    "predict_bleu-4": 88.98009164759723,  
    "predict_model_preparation_time": 0.005,  
    "predict_rouge-1": 92.28347242562928,  
    "predict_rouge-2": 85.85114405034324,  
    "predict_rouge-l": 91.91711041189932,  
    "predict_runtime": 496.0545,  
    "predict_samples_per_second": 1.762,  
    "predict_steps_per_second": 0.881  
    "syntax_error": 67  
- **Parallel-informath with All Formal Language and English Expressions (4,916)**   
**Training (Epochs: 7.5, Steps: 2,303)**  
**Evaluation**  
**On parallel-informath_test_agda_eng (874)**   
    "predict_bleu-4": 88.61289885583525,  
    "predict_model_preparation_time": 0.0051,  
    "predict_rouge-1": 92.53313752860413,  
    "predict_rouge-2": 87.07534725400457,  
    "predict_rouge-l": 92.0372085812357,  
    "predict_runtime": 499.7151,  
    "predict_samples_per_second": 1.749,  
    "predict_steps_per_second": 0.874  
    "syntax_error": 87  
- **Parallel-informath with Only Agda and All Nature Languages Expressions (3,687)**   
**Training (Epochs: 10, Steps: 2,300)**  
**Evaluation**  
**On parallel-informath_test_agda_eng (874)**   
    "predict_bleu-4": 88.30359416475973,  
    "predict_model_preparation_time": 0.005,  
    "predict_rouge-1": 91.04414931350115,  
    "predict_rouge-2": 85.32655148741418,  
    "predict_rouge-l": 91.05881258581235,  
    "predict_runtime": 492.9618,  
    "predict_samples_per_second": 1.773,  
    "predict_steps_per_second": 0.886  
    "syntax_error": 159  
- **Parallel-informath with Only Agda and English Expressions(3,228)**   
**Training (Epochs: 30, Steps: 1,229)**  
**Evaluation**  
**On parallel-informath_test_agda_eng (874)**   
    "predict_bleu-4": 85.71392700228833,  
    "predict_model_preparation_time": 0.0043,  
    "predict_rouge-1": 89.61899542334098,  
    "predict_rouge-2": 85.14092974828374,  
    "predict_rouge-l": 89.52592654462242,  
    "predict_runtime": 518.999,  
    "predict_samples_per_second": 1.684,  
    "predict_steps_per_second": 0.842   
    "syntax_error": 114  

![alt text](/main/results/Qwen2.5-7B/training_logs/parallel-informath_step-loss%20curves.png)  

Score = 0.35 * (1 - ERROR%) * 100 + 0.35 * BLUE-4 + 0.1 * (ROUGE-1 + ROUGE-2 + ROUGE-L)

| No. | Model Name | BLEU-4 | BLEU-4 Δ% | ROUGE-1 | ROUGE-1 Δ% | ROUGE-2 | ROUGE-2 Δ% | ROUGE-L | ROUGE-L Δ% | Syntax Error | Syntax Error % | Score | Score Δ% |
|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
| 1 | Base Line | 51.20 | 0.00% | 75.08 | 0.00% | 53.36 | 0.00% | 60.12 | 0.00% | 294 | 83.76% | 42.46 | 0.00% |
| 2 |M_full_data_v2_full_first1 (38,736) | 98.36 | 92.00% | 99.31 | 32.26% | 98.94 | 85.42% | 98.81 | 64.36% | 14 | 3.90% | 97.77 | 130.26% |
| 3 |M_full_data_v2_eng_first1 (12,912) | 98.63 | 92.57% | 99.48 | 32.49% | 99.13 | 85.77% | 98.99 | 64.66% | 13 | 3.62% | 98.01 | 130.83% |
| 4 |M_full_data_v2_agda_first1 (9,684) | 98.98 | 93.28% | 99.64 | 32.71% | 99.48 | 86.44% | 99.32 | 65.21% | 17 | 4.74% | 97.83 | 130.41% |
| 5 | M_full_data_v2_agda_eng_first1 (3,228) | 98.91 | 93.15% | 99.54 | 32.57% | 99.26 | 86.03% | 99.27 | 65.13% | 19 | 5.29% | 97.58 | 129.82% |
| 6 | M_parallel-informath_full (14,748) | 88.98 | 73.79% | 92.28 | 22.91% | 85.85 | 60.89% | 91.92 | 52.90% | 67 | 7.67% | 90.46 | 113.05% |
| 7 | M_parallel-informath_eng (4,916) | 88.61 | 73.10% | 92.53 | 23.24% | 87.08 | 63.19% | 92.04 | 53.09% | 87 | 9.95% | 89.70 | 111.26% |
| 8 | M_parallel-informath_agda (3,687) | 88.30 | 72.45% | 91.04 | 21.26% | 85.33 | 59.91% | 91.06 | 51.47% | 159 | 18.19% | 86.28 | 103.20% |
| 9 | M_parallel-informath_agda_eng (3,228) | 85.71 | 67.38% | 89.62 | 19.36% | 85.14 | 59.56% | 89.53 | 48.92% | 114 | 13.04% | 86.86 | 104.57% |

By comparison, we can find that:
1. (6 vs 9) Multi-formal/multi-natural joint training can effectively improve various indicators of the model when the number of Agda-Eng samples is small, among which the reduction of Syntax Error is the most obvious.
2. (2 vs 5) Although multi-formal/multi-natural language joint training has little help on the main indicators when the amount of data is large enough, on the contrary, multi-task joint training has learned how to translate other formal/natural language without significantly damaging the performance of main targe (translate between Agda-English), and the training cost (number of steps/time) has nearly no increase.


``` ghc
runghc-9.4.8 CheckAgdaSyntax.hs <exx.agda | grep "ERROR" | tee >(wc -l | awk '{print "ERROR count: " $1}')
```